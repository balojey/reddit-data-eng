[2024-07-14T06:50:05.048+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-14T06:50:05.076+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-14T06:50:03.943991+00:00 [queued]>
[2024-07-14T06:50:05.084+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-14T06:50:03.943991+00:00 [queued]>
[2024-07-14T06:50:05.085+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-07-14T06:50:05.102+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-07-14 06:50:03.943991+00:00
[2024-07-14T06:50:05.110+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=565) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-07-14T06:50:05.112+0000] {standard_task_runner.py:63} INFO - Started process 566 to run task
[2024-07-14T06:50:05.112+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-07-14T06:50:03.943991+00:00', '--job-id', '15', '--raw', '--subdir', 'DAGS_FOLDER/python_dag.py', '--cfg-path', '/tmp/tmp36_4h33b']
[2024-07-14T06:50:05.113+0000] {standard_task_runner.py:91} INFO - Job 15: Subtask reddit_extraction
[2024-07-14T06:50:05.162+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-14T06:50:03.943991+00:00 [running]> on host 12ac62825425
[2024-07-14T06:50:05.261+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Ademola Balogun' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-07-14T06:50:03.943991+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-07-14T06:50:03.943991+00:00'
[2024-07-14T06:50:05.262+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-14T06:50:05.278+0000] {logging_mixin.py:188} INFO - Connected to reddit
[2024-07-14T06:50:05.863+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f93ef45bbc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I work for a big Fortune 100 company in a multiple hats capacity that basically equates to me doing 40% data engineering, 20% analytics engineering, and 40% data analytics or dashboarding. I have to tell you right now that I have seen some amazingly stupid things in my 2 years of engineering so far \n\n\n\n1) I\'ll start with the juiciest one. A table that has over 1,300 columns in it. Yeah, no joke. They were tired of data analysts writing their own queries and using joins in SQL to bring together tables that are separated into normal forms, star schema what have you... So they created a monster table of every column that the person could ever need. This is to be directly queried from, by the way. So it\'s not like it\'s some back end table used for different purposes. This also fed into an analytics cube using Microsoft analysis services, so instead of people writing their own SQL, they can just drag and drop stuff in Excel to create their own reports. Sure, I guess. Seems pretty ridiculous to me, we won\'t train people on proper SQL or simply hire a couple of data analysts to do the job, so we will instead spend hideous amounts of money on extremely inefficient architecture \n\n\n2) tables with no primary indexes or poorly designed ones. There was a ZenDesk ticket database with a couple of tables. They did not have primary index columns on them, so We created an ETL query that used the most absurd join logic I have ever seen in my entire career. We basically used an interval, if someone opened a zendesk ticket within a certain time frame, and another person was assigned it within a certain time frame, then to match them together. there are very obvious reasons why this is a bad idea. The basic idea is that you\'re matching tickets together based on who opened them and who is assigned them. The major problem was that there was simply no guarantee the tickets were being matched together properly because you\'re using time intervals. What happens if John Doe opens a ticket and so does Jane doe 3 seconds later? One agent will be assigned both of those tickets. Took them 9 months to develop a primary index for both tables that could match them together. Why did they not think of that from the beginning? My gosh \n\n\n3) Instead of using a stored procedure and table for reporting, we embedded a 2500-line ETL script directly in Power BI. This script runs daily, making the process extremely resource-intensive, and consuming probably 10x more compute power than needed \n\n\n4) Refusal to allow me to cross-train with other engineers who do more specific data engineering task. Much of them have been outsourced to overseas, so they don\'t want me to "get the wrong idea", since a lot of more advanced and more technical engineering functions are reserved for offshored, cheaper labor. You know, because if I was more intelligent and more skilled, I could probably get an actual 100% data engineering job elsewhere and they don\'t want that you know? They want the multi-tool that can do a little bit of everything', 'author_fullname': 't2_hdeet8zsc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'After 2 years of engineering, I have seen some really stupid things', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e29ges', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 200, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 200, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1720879120.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1720874338.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I work for a big Fortune 100 company in a multiple hats capacity that basically equates to me doing 40% data engineering, 20% analytics engineering, and 40% data analytics or dashboarding. I have to tell you right now that I have seen some amazingly stupid things in my 2 years of engineering so far </p>\n\n<p>1) I&#39;ll start with the juiciest one. A table that has over 1,300 columns in it. Yeah, no joke. They were tired of data analysts writing their own queries and using joins in SQL to bring together tables that are separated into normal forms, star schema what have you... So they created a monster table of every column that the person could ever need. This is to be directly queried from, by the way. So it&#39;s not like it&#39;s some back end table used for different purposes. This also fed into an analytics cube using Microsoft analysis services, so instead of people writing their own SQL, they can just drag and drop stuff in Excel to create their own reports. Sure, I guess. Seems pretty ridiculous to me, we won&#39;t train people on proper SQL or simply hire a couple of data analysts to do the job, so we will instead spend hideous amounts of money on extremely inefficient architecture </p>\n\n<p>2) tables with no primary indexes or poorly designed ones. There was a ZenDesk ticket database with a couple of tables. They did not have primary index columns on them, so We created an ETL query that used the most absurd join logic I have ever seen in my entire career. We basically used an interval, if someone opened a zendesk ticket within a certain time frame, and another person was assigned it within a certain time frame, then to match them together. there are very obvious reasons why this is a bad idea. The basic idea is that you&#39;re matching tickets together based on who opened them and who is assigned them. The major problem was that there was simply no guarantee the tickets were being matched together properly because you&#39;re using time intervals. What happens if John Doe opens a ticket and so does Jane doe 3 seconds later? One agent will be assigned both of those tickets. Took them 9 months to develop a primary index for both tables that could match them together. Why did they not think of that from the beginning? My gosh </p>\n\n<p>3) Instead of using a stored procedure and table for reporting, we embedded a 2500-line ETL script directly in Power BI. This script runs daily, making the process extremely resource-intensive, and consuming probably 10x more compute power than needed </p>\n\n<p>4) Refusal to allow me to cross-train with other engineers who do more specific data engineering task. Much of them have been outsourced to overseas, so they don&#39;t want me to &quot;get the wrong idea&quot;, since a lot of more advanced and more technical engineering functions are reserved for offshored, cheaper labor. You know, because if I was more intelligent and more skilled, I could probably get an actual 100% data engineering job elsewhere and they don&#39;t want that you know? They want the multi-tool that can do a little bit of everything</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e29ges', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='databro92'), 'discussion_type': None, 'num_comments': 91, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e29ges/after_2_years_of_engineering_i_have_seen_some/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e29ges/after_2_years_of_engineering_i_have_seen_some/', 'subreddit_subscribers': 196637, 'created_utc': 1720874338.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-14T06:50:05.863+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-14T06:50:05.864+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 10, in reddit_pipeline
    posts = extract_posts(instance, subreddit, time_filter, limit)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/etls/reddit_etl.py", line 27, in extract_posts
    post = {key: post_dict[key] for key in POST_FIELD}
                                           ^^^^^^^^^^
NameError: name 'POST_FIELD' is not defined
[2024-07-14T06:50:05.879+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, run_id=manual__2024-07-14T06:50:03.943991+00:00, execution_date=20240714T065003, start_date=20240714T065005, end_date=20240714T065005
[2024-07-14T06:50:05.892+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 15 for task reddit_extraction (name 'POST_FIELD' is not defined; 566)
[2024-07-14T06:50:05.928+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-07-14T06:50:05.949+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-07-14T06:50:05.950+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
