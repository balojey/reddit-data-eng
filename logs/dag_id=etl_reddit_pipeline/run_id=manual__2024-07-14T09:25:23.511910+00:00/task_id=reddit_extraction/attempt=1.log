[2024-07-14T09:25:25.625+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-14T09:25:25.650+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-14T09:25:23.511910+00:00 [queued]>
[2024-07-14T09:25:25.657+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-14T09:25:23.511910+00:00 [queued]>
[2024-07-14T09:25:25.658+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-07-14T09:25:25.672+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-07-14 09:25:23.511910+00:00
[2024-07-14T09:25:25.681+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=187) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-07-14T09:25:25.683+0000] {standard_task_runner.py:63} INFO - Started process 189 to run task
[2024-07-14T09:25:25.688+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-07-14T09:25:23.511910+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/python_dag.py', '--cfg-path', '/tmp/tmpmf6csrjd']
[2024-07-14T09:25:25.691+0000] {standard_task_runner.py:91} INFO - Job 9: Subtask reddit_extraction
[2024-07-14T09:25:25.741+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-14T09:25:23.511910+00:00 [running]> on host 906654fcd818
[2024-07-14T09:25:25.852+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Ademola Balogun' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-07-14T09:25:23.511910+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-07-14T09:25:23.511910+00:00'
[2024-07-14T09:25:25.853+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-14T09:25:25.868+0000] {logging_mixin.py:188} INFO - Connected to reddit
[2024-07-14T09:25:26.413+0000] {logging_mixin.py:188} INFO - {'id': '1e29ges', 'title': 'After 2 years of engineering, I have seen some really stupid things', 'selftext': 'I work for a big Fortune 100 company in a multiple hats capacity that basically equates to me doing 40% data engineering, 20% analytics engineering, and 40% data analytics or dashboarding. I have to tell you right now that I have seen some amazingly stupid things in my 2 years of engineering so far \n\n\n\n1) I\'ll start with the juiciest one. A table that has over 1,300 columns in it. Yeah, no joke. They were tired of data analysts writing their own queries and using joins in SQL to bring together tables that are separated into normal forms, star schema what have you... So they created a monster table of every column that the person could ever need. This is to be directly queried from, by the way. So it\'s not like it\'s some back end table used for different purposes. This also fed into an analytics cube using Microsoft analysis services, so instead of people writing their own SQL, they can just drag and drop stuff in Excel to create their own reports. Sure, I guess. Seems pretty ridiculous to me, we won\'t train people on proper SQL or simply hire a couple of data analysts to do the job, so we will instead spend hideous amounts of money on extremely inefficient architecture \n\n\n2) tables with no primary indexes or poorly designed ones. There was a ZenDesk ticket database with a couple of tables. They did not have primary index columns on them, so We created an ETL query that used the most absurd join logic I have ever seen in my entire career. We basically used an interval, if someone opened a zendesk ticket within a certain time frame, and another person was assigned it within a certain time frame, then to match them together. there are very obvious reasons why this is a bad idea. The basic idea is that you\'re matching tickets together based on who opened them and who is assigned them. The major problem was that there was simply no guarantee the tickets were being matched together properly because you\'re using time intervals. What happens if John Doe opens a ticket and so does Jane doe 3 seconds later? One agent will be assigned both of those tickets. Took them 9 months to develop a primary index for both tables that could match them together. Why did they not think of that from the beginning? My gosh \n\n\n3) Instead of using a stored procedure and table for reporting, we embedded a 2500-line ETL script directly in Power BI. This script runs daily, making the process extremely resource-intensive, and consuming probably 10x more compute power than needed \n\n\n4) Refusal to allow me to cross-train with other engineers who do more specific data engineering task. Much of them have been outsourced to overseas, so they don\'t want me to "get the wrong idea", since a lot of more advanced and more technical engineering functions are reserved for offshored, cheaper labor. You know, because if I was more intelligent and more skilled, I could probably get an actual 100% data engineering job elsewhere and they don\'t want that you know? They want the multi-tool that can do a little bit of everything', 'score': 211, 'num_comments': 95, 'author': Redditor(name='databro92'), 'created_utc': 1720874338.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e29ges/after_2_years_of_engineering_i_have_seen_some/', 'upvote_ratio': 0.94, 'over_18': False, 'edited': 1720879120.0, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.414+0000] {logging_mixin.py:188} INFO - {'id': '1e2gpd3', 'title': 'Free Copy of O\'Reilly\'s "Fundamentals of Data Engineering" from Redpanda', 'selftext': 'Just wanted to drop these links to some free DE resources from Redpanda.\n\n[LinkedIn Post](https://www.linkedin.com/posts/redpanda-data_fundamentals-of-data-engineeringpdf-activity-6986109026623238144-166H/)\n\n[Direct Link to PDF](https://go.redpanda.com/hubfs/PDFs/fundamentals-of-data-engineering.pdf?utm_assettype=book&utm_assetname=dataengineering&utm_source=linkedin&utm_medium=pd_social&utm_campaign=data-eng-intl-linkedin&hsa_acc=509225849&hsa_cam=628025613&hsa_grp=213331363&hsa_ad=214663843&hsa_net=linkedin&hsa_ver=3)', 'score': 55, 'num_comments': 1, 'author': Redditor(name='theflyestg'), 'created_utc': 1720893888.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2gpd3/free_copy_of_oreillys_fundamentals_of_data/', 'upvote_ratio': 0.95, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.414+0000] {logging_mixin.py:188} INFO - {'id': '1e2g5ah', 'title': 'Is switching from PySpark to Snowflake a tech stack downgrade?', 'selftext': 'Hey! I\'d appreciate some career advice regarding switching tech stack. For context, the main motivation for moving jobs is a higher salary.\n\nSo I am a data engineer with 3 YOE. I\'ve been offered a DE role in a company that mainly have SQL pipelines, making use of DBT & Snowflake. They do use some Python, but it appears to be limited.\n\nThis is different from my current role which is more on the "software engineering" side of things. Here pretty much everything is done using Python (PySpark, boto3, etc.) in containerised environments. We also maintain the AWS infrastructure / operations ourselves (EMR, RDS, S3, Lambda, IAM, etc.), for which we use Terraform for IAC. \n\nMy understanding is the main appeal of Snowflake is that it removes the technical challenges of managing data infrastructure, thus lowering the barrier to entry. I definitely see the appeal of this from a management perspective, and it seems to be the way the industry is heading.\n\nHowever, would I be lowering my own market value in the long run by moving into an "easier" role? ', 'score': 37, 'num_comments': 37, 'author': Redditor(name='alex_shambles'), 'created_utc': 1720892445.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2g5ah/is_switching_from_pyspark_to_snowflake_a_tech/', 'upvote_ratio': 0.89, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.414+0000] {logging_mixin.py:188} INFO - {'id': '1e2g341', 'title': 'Why can’t databases scale to handle big data?', 'selftext': 'We have a 20TB dataset that we literally cannot load into Oracle or Tableau (via Oracle). The engineers built a spark cluster with S3 storage but no one likes it because it doesn’t behave like a traditional database (inserts, updates, small queries still require a job to be launched). If they wanted a huge database why not just scale Oracle and Tableau? Is there any solution out there that’s just a big database? All my big data user experience is with Hadoop and Spark. I’ve only used traditional databases for sub 100GB sources. Thanks.', 'score': 32, 'num_comments': 43, 'author': Redditor(name='Trick-Interaction396'), 'created_utc': 1720892288.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2g341/why_cant_databases_scale_to_handle_big_data/', 'upvote_ratio': 0.8, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.415+0000] {logging_mixin.py:188} INFO - {'id': '1e2bgvh', 'title': 'Data Engineering + AI', 'selftext': 'What AI stuff is going on in DE?\n\nI’m not talking about using ChatGPT to write code or reformat sql queries (although still helpful)\n\nI’m curious to learn about the newer stuff that deserves more attention', 'score': 26, 'num_comments': 30, 'author': Redditor(name='chrisgarzon19'), 'created_utc': 1720880238.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2bgvh/data_engineering_ai/', 'upvote_ratio': 0.91, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.415+0000] {logging_mixin.py:188} INFO - {'id': '1e2i5pg', 'title': 'Strategy to automate Excel processes/monsters', 'selftext': "Working in a large tech company as a Data Engineer/Analytics Engineer. In my 6 year career I always see Business Teams requesting Data Teams to automate twisted, and complex Excel sheets.\nThese Excel workbooks could have \n- 12+ tabs\n- tables from 10+ different sources\n- These sources come from different domains (Payroll, HR, Finance, FP&A)\n- Twisted formulas relying on everchanging source data. (The source data is prone to change aswell)\n\nGiven that these tables don't yet exist in a Warehouse what is a recommended strategy here? \nDo people prefer to go over the route of building a central Data Model / Warehouse for all these tables? Do people use VBA? \nDo people neglect these type of requests to focus on bigger wins with more accesible data?\nAppreciate any insights or views!", 'score': 20, 'num_comments': 8, 'author': Redditor(name='Mobile-Collection-90'), 'created_utc': 1720897703.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2i5pg/strategy_to_automate_excel_processesmonsters/', 'upvote_ratio': 1.0, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.415+0000] {logging_mixin.py:188} INFO - {'id': '1e2ew9a', 'title': 'C3.ai - where we change our slogan (and domain name) to reflect the latest fads in the data industry ', 'selftext': "That's it, that's the post.\n\nDoes anyone actually use C3 for anything? Do they actually do anything other than force their workforce back to the office for no reason other than to justify their real estate costs and satisfy management that has trouble with object permanence?\n\nWhat's your experience been like?", 'score': 17, 'num_comments': 11, 'author': Redditor(name='JaJ_Judy'), 'created_utc': 1720889250.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2ew9a/c3ai_where_we_change_our_slogan_and_domain_name/', 'upvote_ratio': 0.91, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.415+0000] {logging_mixin.py:188} INFO - {'id': '1e2tlsi', 'title': 'Is this a realistic plan to get into DE?', 'selftext': "I want to get into DE. From what I’ve seen on Reddit, the job market is pretty bad for anything in tech, but especially since I am trying to get into entry level with an unrelated degree. I graduated this May with a BA in Econ and minor in data science from a good university but honestly my technical skillset is not there (yet).\n\nI only have accounting internships and got into an accounting masters program that starts this summer (and costs 35k) but it basically guarantees me a job. I don’t like how the progression in accounting is to play office politics and become a people manager. The coursework I’ve studied feels meaningless. I don’t care about finance anymore and dislike its culture.\n\nHowever, I think it could serve as a good/safe stepping stone into other jobs. I heard that it can be useful to have domain knowledge first, and I can't afford to be unemployed for several years trying to break in. A MS in Statistics or CS would be more useful but I would have to bank on taking pre-reqs and finding entry level experience to have a chance. I figure that if I fail to break into DE, I can fall back on accounting. My plan is to do the masters and do my time in public accounting before getting an accounting job that is DE adjacent (Business intelligence? Financial analyst? ERP?) And then, I will self study along the journey to transition into a real DE role!\n\nIs this a feasible and realistic plan? I haven't been able to get into entry level data analysis jobs. Is the path from accounting to DE a better battle than if I were to try to get into DE now? What certificates would you recommend?\n\nAny advice would be greatly appreciated!", 'score': 7, 'num_comments': 9, 'author': Redditor(name='thepoipoi'), 'created_utc': 1720931428.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2tlsi/is_this_a_realistic_plan_to_get_into_de/', 'upvote_ratio': 0.73, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.415+0000] {logging_mixin.py:188} INFO - {'id': '1e2w7u3', 'title': 'Fresher Data Engineer', 'selftext': 'So I will be joining one of the service based companies next month and my domain is gonna be DE. So as a fresher what tech I should focus on with future perspective apart from python/SQL.\n(I have basic knowledge of python, SQL, Azure)\nAlso on a side note what do you think about DE jobs in future? ', 'score': 5, 'num_comments': 13, 'author': Redditor(name='pirate-king-106'), 'created_utc': 1720941353.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2w7u3/fresher_data_engineer/', 'upvote_ratio': 0.78, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.416+0000] {logging_mixin.py:188} INFO - {'id': '1e2uwq1', 'title': 'What if I completed a project in the insurance industry while working for the same company as an intern for a year? Could I still consider that as one year of experience? -------------------', 'selftext': 'What if I work for the same company as an intern for one year and now as a cloud data engineer so do I have two years of experience or only one? \n\nBecause i worked as "intern"  for year where I work on databricks, pyspark API, and azure services and as of now I\'m working as "cloud data engineer"  in same company from past 1 year where I\'m currently working on snowflake DWH so how much experience should I count it as 1 year or 2 year?\n\nAnd any tips or suggestions what kind of questions will they ask if i go for job switch? I mean like what will they mostly ask about intern project or current project.\n\nI\'m asking because will company accept it as 2 yoe?\n\n', 'score': 4, 'num_comments': 3, 'author': Redditor(name='Infamous_Working6597'), 'created_utc': 1720936278.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2uwq1/what_if_i_completed_a_project_in_the_insurance/', 'upvote_ratio': 1.0, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.416+0000] {logging_mixin.py:188} INFO - {'id': '1e2kybg', 'title': 'Using Data Cubes to build flexible interactive dashboards?', 'selftext': 'Hi All,\n\nI am your run of the mill ETL analyst who does code transformation & data loading in spark-sql, nothing fancy so far. Recently our BU partners requested if they could have a flexible way of exploring data ie use combinations of dimension & metric to understand insights about business.\n\nI spoke with our software engg. folks and they said a client can be built which provides to a fast database and fetch data based on user combinations. Now that got me wondering what would be a suitable database for this problem.\n\nTo give you an insight of the scale of data, **we have 1 major fact table (transactions) which has billions of records, to go with the fact table, we have 3 to 5 dimension tables** which range from having records in thousands to in few cases around millions.\n\nI would imagine a utopian solution would enable users to query for different combination of fact & dimensions to see various business metrics. To this effect, based on my reading I have shortlisted 2 approaches for fast database:\n\n1. Clickhouse database: The client for every client combination would generate a preset query template (which needs to be designed) and surface data, \n\n*  pros :\n   * Fast querying \n   * Already used in our company and some of the data  already lies there (\\~70%)\n   * Flexible to do traditional SQ operations like joins & aggregations\n* Cons:\n   * Would need templatizing the query pattern for potential dimension & metric combinations\n   * Seems like a misfit for these kind of use cases in general\n\n2. Build a Data cube with dimensions:\n\n* Pros:\n   * Seems like a bread & butter solution for cube\n   * Some internal cubes in company exist so not an alien concept\n* Cons:\n   * Not sure how cardinality of data would affect the cube, the transaction data is at item level & as I said it coud run into billions, a dimension with such high cardinality might ean sparse data\n   * Might have to uild multiple cubes (may not be a con honestly) & not sure how performant it would be to join\n   * Cube specific ETL to create pre aggregated tables\n\nI was wondering on what does this community think about this use case solution & goodness of fit of data cube.', 'score': 3, 'num_comments': 0, 'author': Redditor(name='user19911506'), 'created_utc': 1720905187.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2kybg/using_data_cubes_to_build_flexible_interactive/', 'upvote_ratio': 0.81, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.416+0000] {logging_mixin.py:188} INFO - {'id': '1e2cgj2', 'title': 'apache arrow pattern for "piping" dataset from one source to another without reading whole dataset into memory', 'selftext': 'I\'m trying to build an api that will deliver subsets of a dataset stored in postgres as apache parquet files.  I\'m writing the code in python, using psycopg3 to talk to postgres, pyarrow to serialize the objects returned by psycopg3 to parquet, and fastapi to tie everything together.\n\nI\'m struggling to use the pyarrow docs to achieve the serialization step, and I\'m hoping someone can help me better understand the docs, or point me to a resource that might be better suited to my use case than the pyarrow docs seem to be.\n\nThe problem I\'m running into is that all of the examples in [the sections on serialization](https://arrow.apache.org/docs/python/ipc.html#) presume that one has an entire table or collection of record batches in-memory before serializing them.  For instance, here\'s the example from the docs illustrating the use of `RecordBatchStreamWriter`:\n\n    import pyarrow as pa\n    \n    data = [\n        pa.array([1, 2, 3, 4]),\n        pa.array([\'foo\', \'bar\', \'baz\', None]),\n        pa.array([True, None, False, True])\n    ]\n    \n    \n    batch = pa.record_batch(data, names=[\'f0\', \'f1\', \'f2\'])\n    \n    sink = pa.BufferOutputStream()\n    \n    with pa.ipc.new_stream(sink, batch.schema) as writer:\n       for i in range(5):\n          writer.write_batch(batch)\n\nI\'m struggling to apply examples like this because I don\'t want my server to ever hold the entire dataset requested by a client in memory at once.  Instead, I want my server to act as a "pipe".  Specifically, I want to fetch the requested data from postgres in (small) chunks, sending each chunk to the client and removing it from my server\'s memory before it fetching the next chunk to send to the client, with all the chunks tied together and delivered using FastAPI\'s `StreamingResponse`.  Note that this would seem to require sending the schema for the data to the client as either the first or last chunk of the stream, and then sending each record batch chunk WITHOUT the schema as all the other chunks in the stream.  I haven\'t been able to find this particular pattern documented or illustrated in the apache arrow docs.\n\nWhat I\'m hoping r/DATAENGINEERING can do is point me to examples that use pyarrow\'s IPC/SERIALIZATION tools to run a "piping" process along those lines.\n\nI hope this question makes sense.  Let me know if clarification is needed.\n\nAnd feel free to point me to more general resources on how to construct "piping" processes along the lines I have in mind in Python or FastAPI. Perhaps what I need is to understand things at a more abstract level before I concentrate on the implementation details of pyarrow.\n\nThank you all!!!', 'score': 3, 'num_comments': 3, 'author': Redditor(name='rocrocdaddy'), 'created_utc': 1720882932.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2cgj2/apache_arrow_pattern_for_piping_dataset_from_one/', 'upvote_ratio': 1.0, 'over_18': False, 'edited': 1720885297.0, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.416+0000] {logging_mixin.py:188} INFO - {'id': '1e2ud7m', 'title': 'How to create a fast instant spend insight dashboard', 'selftext': 'I live in canada and using td bank. Recently they created an app that gives you spend insights. You spend something it immediately appears in the dash board alongside your other spending for the day. Working with redshift and mysql, i dont think they are capable of providing you with these information almost instantly! I know redshift is pretty powerful database for analytics, heavy joins and group bys, but it cant be this fast. What do they use as database and how is it structured?! Is it cassandra, elastic search or some other db that i am not aware?\n\nAppreciate a bit of details in your answers.\n\nThanks in advance!', 'score': 3, 'num_comments': 3, 'author': Redditor(name='Puzzleheadedanxi'), 'created_utc': 1720934224.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2ud7m/how_to_create_a_fast_instant_spend_insight/', 'upvote_ratio': 0.8, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.416+0000] {logging_mixin.py:188} INFO - {'id': '1e2ta0w', 'title': 'Genome data pipelines ', 'selftext': 'Hi,\n\nI am trying to set up/create end to end data infrastructure. For large amounts of data.\nI don’t know where to get started. There are FastQ and FastA files so far.', 'score': 3, 'num_comments': 3, 'author': Redditor(name='Next-Nail9394'), 'created_utc': 1720930246.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2ta0w/genome_data_pipelines/', 'upvote_ratio': 0.72, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.417+0000] {logging_mixin.py:188} INFO - {'id': '1e2bq4s', 'title': "Gretel's synthetic data outperformed GPT-4 by 25.6% and Llama3-70b by 48.1%", 'selftext': "Gretel Navigator’s synthetic data generation outperformed OpenAI's GPT-4 by 25.6%, surpassed Llama3-70b by 48.1%, and exceeded human expert-curated data by 73.6%. [https://gretel.ai/blog/how-to-create-high-quality-synthetic-data-for-fine-tuning-llms](https://gretel.ai/blog/how-to-create-high-quality-synthetic-data-for-fine-tuning-llms)", 'score': 0, 'num_comments': 0, 'author': Redditor(name='alig80'), 'created_utc': 1720880945.0, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e2bq4s/gretels_synthetic_data_outperformed_gpt4_by_256/', 'upvote_ratio': 0.4, 'over_18': False, 'edited': False, 'spoiler': False, 'stickied': False}
[2024-07-14T09:25:26.429+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-07-14T09:25:26.430+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-14T09:25:26.440+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, run_id=manual__2024-07-14T09:25:23.511910+00:00, execution_date=20240714T092523, start_date=20240714T092525, end_date=20240714T092526
[2024-07-14T09:25:26.503+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-07-14T09:25:26.526+0000] {taskinstance.py:3503} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-07-14T09:25:26.527+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
